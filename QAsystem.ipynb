{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    LOCATION       0.00      0.00      0.00        36\n",
      "      NUMBER       0.59      0.09      0.15       116\n",
      "ORGANIZATION       0.00      0.00      0.00        26\n",
      "       OTHER       0.60      0.98      0.74       292\n",
      "      PERSON       0.00      0.00      0.00        30\n",
      "\n",
      " avg / total       0.48      0.59      0.47       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PartI: Question Processing\n",
    "# - Answer Type Detection\n",
    "# -- build a supervised ML classifier to predict the answer type for each question in the test set\n",
    "# -- use questions in train dataset as the training questions\n",
    "# -- use ner(answer) as the labelled class *\n",
    "# -- use POS tag of each word in the question as the features \n",
    "# -- // alternatives: tokens in the question(BOW) ; NER of each word in question ; synset ID of each word in question\n",
    "# - Query Formulation\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "stanford_ner_tagger = StanfordNERTagger('/Users/zhanglufan/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/Users/zhanglufan/stanford-ner/stanford-ner.jar')\n",
    "stanford_pos_tagger = StanfordPOSTagger('/Users/zhanglufan/stanford-pos/models/english-bidirectional-distsim.tagger','/Users/zhanglufan/stanford-pos/stanford-postagger.jar')\n",
    "#print st.tag('what is Karin\\'s birthday?'.split()) \n",
    "#print stanford_pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split())\n",
    "\n",
    "# build classifier\n",
    "### get class lable\n",
    "with open('/Users/zhanglufan/QA_dev.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "training_data = []\n",
    "training_answers = []\n",
    "training_questions = []\n",
    "training_answers_ner = [] # (answer,NER type)\n",
    "#label_list = []\n",
    "for ques_article in data:\n",
    "    for each in ques_article['qa']:\n",
    "        training_answers.append(each['answer']) # punctuations are useful in determining correct answers-keep punctuations\n",
    "        training_questions.append(each['question'])\n",
    "        training_data.append((each['question'],each['answer']))\n",
    "#print training_answers[:30]\n",
    "#print training_questions[:30]\n",
    "#print training_data[:30]\n",
    "#print training_answers_ner[:30]\n",
    "def find_number(answer):\n",
    "    num = re.match(\"\\d+\",answer)\n",
    "    if num:\n",
    "        return True\n",
    "        #training_answers_ner[answer] = 'NUMBER'\n",
    "def find_organization(ner_list):\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'ORGANIZATION':\n",
    "                return True\n",
    "                break   \n",
    "    \n",
    "def find_person(ner_list):\n",
    "    #if 'PERSON' in ner_list.values():\n",
    "        #return True\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'PERSON':\n",
    "                return True\n",
    "                break           \n",
    "#find_person('William Herschel')\n",
    "#print '======'\n",
    "def find_location(ner_list):\n",
    "    #ner_list = set(ner_list)\n",
    "    #if 'LOCATION' in ner_list:\n",
    "        #return True\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'LOCATION':\n",
    "                return True\n",
    "                break        \n",
    "\n",
    "#print stanford_ner_tagger.tag('the Royal Society of London'.split())    \n",
    "#print \"------------\"\n",
    "#lll = [ ['red', 'dog'],['780','nm'],['Infrared','astronomy'] ]\n",
    "#print '----------'\n",
    "#print 'active near-infrared illumination'.split() ['active', 'near-infrared', 'illumination']\n",
    "#print stanford_ner_tagger.tag_sents(['active near-infrared illumination'.split()]) [[(u'active', u'O'), (u'near-infrared', u'O'), (u'illumination', u'O')]]\n",
    "def get_label_array(training_answers):\n",
    "    for answer in training_answers:\n",
    "        ner_list = stanford_ner_tagger.tag_sents([answer.split()])\n",
    "        #print ner_list\n",
    "        #ner_list = dict(ner_list)\n",
    "        if find_number(answer):\n",
    "            training_answers_ner.append((answer,'NUMBER'))\n",
    "        elif find_organization(ner_list):\n",
    "            training_answers_ner.append((answer,'ORGANIZATION'))\n",
    "        elif find_location(ner_list):\n",
    "            training_answers_ner.append((answer,'LOCATION'))\n",
    "        elif find_person(ner_list):\n",
    "            training_answers_ner.append((answer,'PERSON'))\n",
    "        else:\n",
    "            training_answers_ner.append((answer,'OTHER'))\n",
    "    return training_answers_ner\n",
    "\n",
    "def get_label(answer):\n",
    "    ner_list = stanford_ner_tagger.tag_sents([answer.split()])\n",
    "    if find_number(answer):\n",
    "        return 'NUMBER'\n",
    "            #training_answers_ner.append((answer,'NUMBER'))\n",
    "    elif find_organization(ner_list):\n",
    "        return 'ORGANIZATION'\n",
    "            #training_answers_ner.append((answer,'ORGANIZATION'))\n",
    "    elif find_location(ner_list):\n",
    "        return 'LOCATION'\n",
    "            #training_answers_ner.append((answer,'LOCATION'))\n",
    "    elif find_person(ner_list):\n",
    "        return 'PERSON'\n",
    "            #training_answers_ner.append((answer,'PERSON'))\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "            #training_answers_ner.append((answer,'OTHER'))\n",
    "    #return training_answers_ner\n",
    "\n",
    "#all_labels = get_lable_array(training_answers[:500])\n",
    "#print get_label('the Royal Society of London')\n",
    "### build feature matrix\n",
    "#print stanford_pos_tagger.tag('Which city is the capital of China ?'.split())\n",
    "#print stanford_ner_tagger.tag('Which city is the capital of China ?'.split())\n",
    "#print get_label('Beijing')\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "\n",
    "#word_features = []\n",
    "cfdist = ConditionalFreqDist() #for condition is NN,NNP\n",
    "def get_all_words():\n",
    "    #all_words = []\n",
    "    for question in training_questions:\n",
    "        for word in word_tokenize(question):\n",
    "            condition = len(word)\n",
    "            cfdist[condition][word] += 1\n",
    "    \n",
    "    return cfdist\n",
    "\n",
    "def get_all_words2():\n",
    "    fdist = FreqDist()\n",
    "    for question in training_questions:\n",
    "        for word in word_tokenize(question):\n",
    "            fdist[word] += 1\n",
    "    return fdist\n",
    "        \n",
    "\n",
    "#print word_features\n",
    "def feature_extractor(question):\n",
    "    features = {}\n",
    "    doc_words = set(question.split())\n",
    "    a = get_all_words2()    \n",
    "    word_features = list(a)[:200] \n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in doc_words)\n",
    "        #features[word] = a[word]\n",
    "    return features\n",
    "#print feature_extractor('red')        \n",
    "\n",
    "def prepare_feature_data(feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for q,a in training_data[:500]:\n",
    "        feature_dict = feature_extractor(q)   \n",
    "        feature_matrix.append(feature_dict)\n",
    "        classifications.append(get_label(a))\n",
    "     \n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset,classifications\n",
    "\n",
    "dataset,classifications = prepare_feature_data(feature_extractor)\n",
    "#print dataset._shape\n",
    "clf = RandomForestClassifier()\n",
    "predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "def check_results(predictions, classifications):\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(classifications,predictions)\n",
    "    print classification_report(classifications,predictions)\n",
    "    \n",
    "check_results(predictions, classifications)\n",
    "#featuresets = [(document_features(get_document(q)), get_label(a)) for (q,a) in training_data]\n",
    "#print featuresets\n",
    "#train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "#classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PartII: Passage Retrieval\n",
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "\n",
    "with open('/Users/zhanglufan/QA_dev.json') as data_file2:\n",
    "    data2 = json.load(data_file2)\n",
    "dev_data = data2[:1] \n",
    "dev_answers = []\n",
    "dev_questions = []\n",
    "dev_sentences = {}\n",
    "#print dev_data\n",
    "#for ques_article in dev_data:\n",
    "    #print ques_article\n",
    "    #for each in ques_article['qa']:\n",
    "        #dev_answers.append(each['answer']) \n",
    "        #dev_questions.append(each['question'])\n",
    "def get_questions(i):\n",
    "    for q in dev_data[i]['qa']:\n",
    "        dev_questions.append(q['question'])\n",
    "    return dev_questions\n",
    "    \n",
    "get_questions(0)    \n",
    "def get_doc(i):\n",
    "    index = 0\n",
    "    #sent_dict = {}\n",
    "    for sent in dev_data[i]['sentences']:\n",
    "        dev_sentences[index] = sent\n",
    "        #sent_dict[index] = sent\n",
    "        #dev_sentences.append(sent_dict)\n",
    "        index += 1\n",
    "    return dev_sentences\n",
    "get_doc(0)\n",
    "#print ab\n",
    "#print dev_questions\n",
    "#print dev_sentences\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords:\n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs\n",
    "#print extract_term_freqs(data2[0]['qa'][0]['question'])\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs # how many doc contains this term\n",
    "\n",
    "doc_term_freqs = {}\n",
    "for docid,sent in dev_sentences.items():\n",
    "    #print q\n",
    "    term_freqs = extract_term_freqs(sent)\n",
    "    doc_term_freqs[docid] = term_freqs\n",
    "M = len(doc_term_freqs)\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n",
    "#print doc_term_freqs\n",
    "\n",
    "# build inverted index\n",
    "vsm_inverted_index = defaultdict(list)\n",
    "for docid, term_freqs in doc_term_freqs.items():\n",
    "    N = sum(term_freqs.values())\n",
    "    length = 0\n",
    "    \n",
    "    # find tf*idf values and accumulate sum of squares \n",
    "    tfidf_values = []\n",
    "    for term, count in term_freqs.items():\n",
    "        tfidf = float(count) / N * log(M / float(doc_freqs[term]))\n",
    "        tfidf_values.append((term, tfidf))\n",
    "        length += tfidf ** 2\n",
    "\n",
    "    # normalise documents by length and insert into index\n",
    "    length = length ** 0.5\n",
    "    for term, tfidf in tfidf_values:\n",
    "        # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "        vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "        \n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for term, docids in vsm_inverted_index.items():\n",
    "    docids.sort()\n",
    "    \n",
    "def query_vsm(query, index, k=1):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "#results = query_vsm([stemmer.stem(term.lower()) for term in 'What technology is used by night-vision devices?'.split()], vsm_inverted_index)\n",
    "#print results\n",
    "\n",
    "\n",
    "def get_relevant_sentences(question,query):\n",
    "    results = query([stemmer.stem(term.lower()) for term in question.split()], vsm_inverted_index)\n",
    "    return results\n",
    "#print get_relevant_sentences('What technology is used by night-vision devices?',query_vsm)\n",
    "#dev_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PartIII: Answer Extraction\n",
    "# process the relevant sentence \n",
    "# to get corresponding answer type entity\n",
    "\n",
    "#get relevant sentences\n",
    "for question in dev_questions:\n",
    "    sent_candidates = get_relevant_sentences(question,query_vsm)\n",
    "    #print sent_candidates\n",
    "\n",
    "def get_possible_answer(question,sent):\n",
    "    answer_ner_list = []\n",
    "    sentence = dev_sentences[sent[0][0]]\n",
    "    answer_ner_list = stanford_ner_tagger.tag_sents([sentence.split()])\n",
    "    predict_answer_type = get_label(question)\n",
    "    return answer_ner_list, predict_answer_type\n",
    "q = \"What notable warming effect does the presence of infrared absorbers contribute to?\"\n",
    "an = get_relevant_sentences(q,query_vsm)\n",
    "#print an\n",
    "\n",
    "def retrieve_answer(sent_candidates,answer_type):\n",
    "    all_possible_answers = []\n",
    "    for each in sent_candidates:\n",
    "        sent = dev_sentences[each[0]]\n",
    "        answer_ner_list = stanford_ner_tagger.tag_sents([sent.split()])\n",
    "        answer_pos_list = stanford_pos_tagger.tag_sents([sent.split()])\n",
    "        \n",
    "\n",
    "sent = dev_sentences[93]\n",
    "answer_ner_list = stanford_ner_tagger.tag_sents([sent.split()])\n",
    "answer_pos_list = stanford_pos_tagger.tag_sents([sent.split()])\n",
    "#print answer_ner_list, answer_pos_list\n",
    "\n",
    "# basic assumption that answer term should not be explicitly in the question\n",
    "def check_question(question,pos_list):\n",
    "    for each in nltk.word_tokenize(question):\n",
    "        #print dict(pos_list[0]).keys()\n",
    "        if each in dict(pos_list[0]).keys():\n",
    "            return False\n",
    "            break\n",
    "        else:\n",
    "            #print each\n",
    "            return True\n",
    "#pos = stanford_pos_tagger.tag_sents(['new applicatons are used money'.split()])\n",
    "#print check_question('What reflectance is measured?',pos)    \n",
    "def get_rid_q(question,sent):\n",
    "    s_list = set(nltk.word_tokenize(sent))\n",
    "    for each in nltk.word_tokenize(question):\n",
    "        if each in s_list:\n",
    "            s_list.remove(each)\n",
    "    #new_sent = list(s_list)\n",
    "    return s_list#new_sent\n",
    "#print get_rid_q('What reflectance is measured?','The reflectance of light is measured')    \n",
    "\n",
    "def get_first_nn(q,pos_list):\n",
    "    for each in pos_list[0]:\n",
    "        if each[0] not in set(nltk.word_tokenize(q)):\n",
    "            if 'NN' == each[1] or 'NNP' == each[1]:\n",
    "                return each[0]\n",
    "                break\n",
    "\n",
    "            \n",
    "#print get_first_nn('Along with industrial and medical, in what applications is infrared radiation used?',pos)\n",
    "#print get_label('What technology is used by night-vision devices?')\n",
    "def answer_type_filter(ans_type, ner_list):\n",
    "    for each in ner_list[0]:\n",
    "        if ans_type == each[1]:\n",
    "            return each[0]\n",
    "            #print each[0]\n",
    "            break\n",
    "            \n",
    "def get_answer(question):\n",
    "    sent_candidate = get_relevant_sentences(question,query_vsm)\n",
    "    #print sent_candidate\n",
    "    sent = dev_sentences[sent_candidate[0][0]]\n",
    "    #print sent\n",
    "    #sent = get_rid_q(question,sent) \n",
    "    #print sent\n",
    "    answer_ner_list = stanford_ner_tagger.tag_sents([sent.split()])\n",
    "    #print answer_ner_list\n",
    "    answer_pos_list = stanford_pos_tagger.tag_sents([sent.split()])\n",
    "    predict_answer_type = get_label(question)\n",
    "    #print predict_answer_type\n",
    "    if predict_answer_type == 'OTHER':\n",
    "        #print '-----'\n",
    "        return get_first_nn(question,answer_pos_list)\n",
    "    else:\n",
    "        print '--------'\n",
    "        return answer_type_filter(predict_answer_type, answer_ner_list)\n",
    "            \n",
    "#print get_answer('What reflectance is measured from a semiconductor wafer\\'s surface to determine the index of refraction?')     \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# get answer for dev_dataset\n",
    "for q in dev_questions:\n",
    "    print q, get_answer(q)\n",
    "#print dev_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
