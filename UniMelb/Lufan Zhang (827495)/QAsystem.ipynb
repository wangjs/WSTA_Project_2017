{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PartI: Question Processing\n",
    "# - Answer Type Detection\n",
    "# -- build a supervised ML classifier to predict the answer type for each question in the test set\n",
    "# -- use questions in train dataset as the training questions\n",
    "# -- use ner(answer) as the labelled class *\n",
    "# -- use POS tag of each word in the question as the features \n",
    "# -- // alternatives: tokens in the question(BOW) ; NER of each word in question ; synset ID of each word in question\n",
    "# - Query Formulation\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from time import ctime\n",
    "from math import log\n",
    "from nltk import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#printing start time of the script\n",
    "#print(\"Start Time:\",ctime())\n",
    "stanford_ner_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "stanford_pos_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "# build classifier\n",
    "### get class lable\n",
    "with open('/Users/zhanglufan/QA_train.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "training_data = []\n",
    "training_answers = []\n",
    "training_questions = []\n",
    "training_answers_ner = [] # (answer,NER type)\n",
    "\n",
    "# getting list of english punctuation marks to clean out sentences\n",
    "PunctuationExclude = set(string.punctuation)\n",
    "PunctuationExclude.remove(',')\n",
    "PunctuationExclude.remove('-')\n",
    "PunctuationExclude.remove('.')\n",
    "PunctuationExclude.remove('\\'')\n",
    "PunctuationExclude.remove('%')\n",
    "\n",
    "for ques_article in data:\n",
    "    for each in ques_article['qa']:\n",
    "        training_answers.append(each['answer']) # punctuations are useful in determining correct answers-keep punctuations\n",
    "        training_questions.append(each['question'])\n",
    "        training_data.append((each['question'],each['answer']))\n",
    "#print training_answers[:30]\n",
    "#print training_questions[:30]\n",
    "#print training_data[:30]\n",
    "#print training_answers_ner[:30]\n",
    "def find_number(answer):\n",
    "    num = re.match(\"\\d+\",answer)\n",
    "    if num:\n",
    "        return True\n",
    "        #training_answers_ner[answer] = 'NUMBER'\n",
    "def find_organization(ner_list):\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'ORGANIZATION':\n",
    "                return True\n",
    "                break   \n",
    "    \n",
    "def find_person(ner_list):\n",
    "    #if 'PERSON' in ner_list.values():\n",
    "        #return True\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'PERSON':\n",
    "                return True\n",
    "                break           \n",
    "#find_person('William Herschel')\n",
    "#print '======'\n",
    "def find_location(ner_list):\n",
    "    #ner_list = set(ner_list)\n",
    "    #if 'LOCATION' in ner_list:\n",
    "        #return True\n",
    "    for each in ner_list:\n",
    "        for e in each:\n",
    "            if e[1] == 'LOCATION':\n",
    "                return True\n",
    "                break        \n",
    "\n",
    "def get_label_array(training_answers):\n",
    "    for answer in training_answers:\n",
    "        ner_list = stanford_ner_tagger.tag_sents([answer.split()])\n",
    "        #print ner_list\n",
    "        #ner_list = dict(ner_list)\n",
    "        if find_number(answer):\n",
    "            training_answers_ner.append((answer,'NUMBER'))\n",
    "        elif find_organization(ner_list):\n",
    "            training_answers_ner.append((answer,'ORGANIZATION'))\n",
    "        elif find_location(ner_list):\n",
    "            training_answers_ner.append((answer,'LOCATION'))\n",
    "        elif find_person(ner_list):\n",
    "            training_answers_ner.append((answer,'PERSON'))\n",
    "        else:\n",
    "            training_answers_ner.append((answer,'OTHER'))\n",
    "    return training_answers_ner\n",
    "\n",
    "def get_label(answer):\n",
    "    ner_list = stanford_ner_tagger.tag_sents([answer.split()])\n",
    "    if find_number(answer):\n",
    "        return 'NUMBER'\n",
    "            #training_answers_ner.append((answer,'NUMBER'))\n",
    "    elif find_organization(ner_list):\n",
    "        return 'ORGANIZATION'\n",
    "            #training_answers_ner.append((answer,'ORGANIZATION'))\n",
    "    elif find_location(ner_list):\n",
    "        return 'LOCATION'\n",
    "            #training_answers_ner.append((answer,'LOCATION'))\n",
    "    elif find_person(ner_list):\n",
    "        return 'PERSON'\n",
    "            #training_answers_ner.append((answer,'PERSON'))\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "            #training_answers_ner.append((answer,'OTHER'))\n",
    "    #return training_answers_ner\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "cfdist = ConditionalFreqDist() #for condition is NN,NNP\n",
    "def get_all_words():\n",
    "    #all_words = []\n",
    "    for question in training_questions:\n",
    "        for word in word_tokenize(question):\n",
    "            condition = len(word)\n",
    "            cfdist[condition][word] += 1  \n",
    "    return cfdist\n",
    "\n",
    "def get_all_words2():\n",
    "    fdist = FreqDist()\n",
    "    for question in training_questions:\n",
    "        for word in word_tokenize(question):\n",
    "            fdist[word] += 1\n",
    "    return fdist\n",
    "        \n",
    "\n",
    "#print word_features\n",
    "def feature_extractor(question):\n",
    "    #print question\n",
    "    features = {}\n",
    "    doc_words = set(question.split())\n",
    "    a = get_all_words2()    \n",
    "    word_features = list(a)[:200] \n",
    "    for word in word_features:\n",
    "        #features['contains({})'.format(word)] = (word in doc_words)\n",
    "        features[word] = a[word]\n",
    "    return features\n",
    "#print feature_extractor('red')        \n",
    "\n",
    "def prepare_feature_data(feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for q,a in training_data[:500]:\n",
    "        feature_dict = feature_extractor(q)   \n",
    "        feature_matrix.append(feature_dict)\n",
    "        classifications.append(get_label(a))\n",
    "     \n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset,classifications\n",
    "\n",
    "dataset,classifications = prepare_feature_data(feature_extractor)\n",
    "#print dataset._shape\n",
    "clf = RandomForestClassifier()\n",
    "predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "def check_results(predictions, classifications):\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(classifications,predictions)\n",
    "    print classification_report(classifications,predictions)\n",
    "    \n",
    "#check_results(predictions, classifications)\n",
    "#featuresets = [(document_features(get_document(q)), get_label(a)) for (q,a) in training_data]\n",
    "#print featuresets\n",
    "#train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "#classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PartII: Passage Retrieval\n",
    "from collections import defaultdict, Counter\n",
    "from math import log\n",
    "\n",
    "with open('/Users/zhanglufan/QA_dev.json') as data_file2:\n",
    "    data2 = json.load(data_file2)\n",
    "dev_data = data2[:10] \n",
    "dev_answers = []\n",
    "dev_questions = []\n",
    "dev_sentences = {}\n",
    "dev_q_a = []\n",
    "\n",
    "def get_ques_ans(i):\n",
    "    for q in dev_data[i]['qa']:\n",
    "        dev_q_a.append((q['question'],q['answer']))\n",
    "    return dev_q_a\n",
    "get_ques_ans(0)\n",
    "\n",
    "def get_questions(i):\n",
    "    for q in dev_data[i]['qa']:\n",
    "        dev_questions.append(q['question'])\n",
    "    return dev_questions    \n",
    "get_questions(0)\n",
    "\n",
    "def get_doc(i):\n",
    "    index = 0\n",
    "    #sent_dict = {}\n",
    "    for sent in dev_data[i]['sentences']:\n",
    "        dev_sentences[index] = sent\n",
    "        #sent_dict[index] = sent\n",
    "        #dev_sentences.append(sent_dict)\n",
    "        index += 1\n",
    "    return dev_sentences\n",
    "get_doc(0)\n",
    "\n",
    "#print dev_questions\n",
    "#print dev_sentences\n",
    "\n",
    "# getting the list of english stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.remove('the') ## After the error analysis of the results I realised that many answers have these words i.e. The President\n",
    "stopwords.remove('of') ## So will not exclude these\n",
    "\n",
    "stopwordsAll = set(nltk.corpus.stopwords.words('english'))\n",
    "#stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords:\n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs\n",
    "#print extract_term_freqs(data2[0]['qa'][0]['question'])\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs # how many doc contains this term\n",
    "\n",
    "doc_term_freqs = {}\n",
    "for docid,sent in dev_sentences.items():\n",
    "    #print q\n",
    "    term_freqs = extract_term_freqs(sent)\n",
    "    doc_term_freqs[docid] = term_freqs\n",
    "M = len(doc_term_freqs)\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n",
    "#print doc_term_freqs\n",
    "\n",
    "# build inverted index\n",
    "vsm_inverted_index = defaultdict(list)\n",
    "for docid, term_freqs in doc_term_freqs.items():\n",
    "    N = sum(term_freqs.values())\n",
    "    length = 0\n",
    "    \n",
    "    # find tf*idf values and accumulate sum of squares \n",
    "    tfidf_values = []\n",
    "    for term, count in term_freqs.items():\n",
    "        tfidf = float(count) / N * log(M / float(doc_freqs[term]))\n",
    "        tfidf_values.append((term, tfidf))\n",
    "        length += tfidf ** 2\n",
    "\n",
    "    # normalise documents by length and insert into index\n",
    "    length = length ** 0.5\n",
    "    for term, tfidf in tfidf_values:\n",
    "        # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "        vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "        \n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for term, docids in vsm_inverted_index.items():\n",
    "    docids.sort()\n",
    "    \n",
    "def query_vsm(query, index, k=1):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "#results = query_vsm([stemmer.stem(term.lower()) for term in 'What technology is used by night-vision devices?'.split()], vsm_inverted_index)\n",
    "#print results\n",
    "\n",
    "\n",
    "def get_relevant_sentences(question,query):\n",
    "    results = query([stemmer.stem(term.lower()) for term in question.split()], vsm_inverted_index)\n",
    "    return results\n",
    "#print get_relevant_sentences('What technology is used by night-vision devices?',query_vsm)\n",
    "#dev_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PartIII: Answer Extraction\n",
    "# process the relevant sentence \n",
    "# to get corresponding answer type entity\n",
    "from nltk import ngrams\n",
    "#get relevant sentences\n",
    "for question in dev_questions:\n",
    "    sent_candidates = get_relevant_sentences(question,query_vsm)\n",
    "    #print sent_candidates\n",
    "\n",
    "def get_possible_answer(question,sent):\n",
    "    answer_ner_list = []\n",
    "    sentence = dev_sentences[sent[0][0]]\n",
    "    answer_ner_list = stanford_ner_tagger.tag_sents([sentence.split()])\n",
    "    predict_answer_type = get_label(question)\n",
    "    return answer_ner_list, predict_answer_type\n",
    "q = \"What notable warming effect does the presence of infrared absorbers contribute to?\"\n",
    "an = get_relevant_sentences(q,query_vsm)\n",
    "#print an\n",
    "\n",
    "def retrieve_answer(sent_candidates,answer_type):\n",
    "    all_possible_answers = []\n",
    "    for each in sent_candidates:\n",
    "        sent = dev_sentences[each[0]]\n",
    "        answer_ner_list = stanford_ner_tagger.tag_sents([sent.split()])\n",
    "        answer_pos_list = stanford_pos_tagger.tag_sents([sent.split()])\n",
    "        \n",
    "\n",
    "sent = dev_sentences[93]\n",
    "answer_ner_list = stanford_ner_tagger.tag_sents([sent.split()])\n",
    "answer_pos_list = stanford_pos_tagger.tag_sents([sent.split()])\n",
    "#print answer_ner_list, answer_pos_list\n",
    "\n",
    "# basic assumption that answer term should not be explicitly in the question\n",
    "def check_question(question,pos_list):\n",
    "    for each in nltk.word_tokenize(question):\n",
    "        #print dict(pos_list[0]).keys()\n",
    "        if each in dict(pos_list[0]).keys():\n",
    "            return False\n",
    "            break\n",
    "        else:\n",
    "            #print each\n",
    "            return True\n",
    "#pos = stanford_pos_tagger.tag_sents(['new applicatons are used money'.split()])\n",
    "#print check_question('What reflectance is measured?',pos)    \n",
    "def get_rid_q(question,sent):\n",
    "    s_list = set(nltk.word_tokenize(sent))\n",
    "    for each in nltk.word_tokenize(question):\n",
    "        if each in s_list:\n",
    "            s_list.remove(each)\n",
    "    #new_sent = list(s_list)\n",
    "    return s_list#new_sent\n",
    "   \n",
    "\n",
    "def get_first_nn(q,pos_list):\n",
    "    for each in pos_list[0]:\n",
    "        if each[0] not in set(nltk.word_tokenize(q)):\n",
    "            if 'NN' == each[1] or 'NNP' == each[1]:\n",
    "                return each[0]\n",
    "                break\n",
    "\n",
    "def check_nn(ngram_pos_list):\n",
    "    counter = 0\n",
    "    for each in ngram_pos_list[0]:\n",
    "        if 'NN' == each[1] or 'NNP' == each[1]:\n",
    "            counter += 1\n",
    "    return counter\n",
    "    \n",
    "def get_ngram_nn(n,q,sent):\n",
    "    ng = ngrams(sent.split(), n)\n",
    "    nn_dict = dict()\n",
    "    #returned_ans = []\n",
    "    for grams in ng:\n",
    "        ngram_pos_list = stanford_pos_tagger.tag_sents([grams])\n",
    "        #print ngram_pos_list\n",
    "        c_nn = check_nn(ngram_pos_list)\n",
    "        #print c_nn\n",
    "        nn_dict[c_nn] = ngram_pos_list[0]       \n",
    "    result = [value for (key, value) in sorted(nn_dict.items(), reverse=True)]#sorted(nn_dict, key=nn_dict.__getitem__, reverse=True)[0]\n",
    "    final = list(zip(*result[0]))[0]\n",
    "    return final\n",
    "        \n",
    "        \n",
    "print get_ngram_nn(3,'What reflectance is measured?','The reflectance of light is measured')         \n",
    "#print get_first_nn('Along with industrial and medical, in what applications is infrared radiation used?',pos)\n",
    "#print get_label('What technology is used by night-vision devices?')\n",
    "#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#note: by locating index of the same term occurring in both q&a, we can further explore number of words we return\n",
    "def find_match(q,s):\n",
    "    ques = set([stemmer.stem(w.lower()) for w in q])\n",
    "    s = [stemmer.stem(w.lower()) for w in s]\n",
    "    s_index = []\n",
    "    for e in s:\n",
    "        if e in ques:\n",
    "            s_index.append(s.index(e))\n",
    "    return s_index\n",
    "def find_nn(s,s_index):\n",
    "    s = s[s_index[-1]+1:]\n",
    "    #print s\n",
    "    s_pos = stanford_pos_tagger.tag_sents([s])\n",
    "    #print s_pos\n",
    "    for each in s_pos[0]:\n",
    "        if 'NN' == each[1] or 'NNP' == each[1]:\n",
    "            index = s.index(each[0])\n",
    "            #print index\n",
    "            return s[:index+1]\n",
    "            break\n",
    "#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def answer_type_filter(ans_type, ner_list):\n",
    "    for each in ner_list[0]:\n",
    "        if ans_type == each[1]:\n",
    "            return each[0]\n",
    "            #print each[0]\n",
    "            break\n",
    "            \n",
    "def get_answer(question):\n",
    "    questionText = ''.join(w for w in question if w not in PunctuationExclude) ######\n",
    "    #print questionText\n",
    "    sent_candidate = get_relevant_sentences(questionText,query_vsm)\n",
    "    #print sent_candidate\n",
    "    sent = dev_sentences[sent_candidate[0][0]]\n",
    "    sent2 = ''.join(w for w in sent if w not in PunctuationExclude)\n",
    "    sent2 = sent2.replace(\",\", \" ,\")\n",
    "    sent2 = sent2.replace(\".\", \" .\")\n",
    "    #print sent\n",
    "    #sent = get_rid_q(question,sent) \n",
    "    #print sent\n",
    "    answer_ner_list = stanford_ner_tagger.tag_sents([sent2.split()])\n",
    "    #print answer_ner_list\n",
    "    answer_pos_list = stanford_pos_tagger.tag_sents([sent2.split()])\n",
    "    predict_answer_type = get_label(questionText)\n",
    "    #print predict_answer_type\n",
    "    if predict_answer_type == 'OTHER':\n",
    "        #print '-----'\n",
    "        return get_first_nn(questionText,answer_pos_list)\n",
    "    else:\n",
    "        print '--------'\n",
    "        return answer_type_filter(predict_answer_type, answer_ner_list)\n",
    "            \n",
    "#print get_answer('What reflectance is measured from a semiconductor wafer\\'s surface to determine the index of refraction?')     \n",
    "\n",
    "def get_answer2(question):\n",
    "    questionText = ''.join(w for w in question if w not in PunctuationExclude) ######\n",
    "    questionText = questionText.replace(\",\", \" ,\")\n",
    "    questionText = questionText.replace(\".\", \" .\")\n",
    "    questionText = questionText.replace(\"?\", \" ?\")\n",
    "    #print questionText\n",
    "    sent_candidate = get_relevant_sentences(questionText,query_vsm)\n",
    "    #print sent_candidate\n",
    "    sent = dev_sentences[sent_candidate[0][0]]\n",
    "    sent2 = ''.join(w for w in sent if w not in PunctuationExclude)\n",
    "    sent2 = sent2.replace(\",\", \" ,\")\n",
    "    sent2 = sent2.replace(\".\", \" .\")\n",
    "    answer_ner_list = stanford_ner_tagger.tag_sents([sent2.split()])\n",
    "    #print answer_ner_list\n",
    "    answer_pos_list = stanford_pos_tagger.tag_sents([sent2.split()])\n",
    "    predict_answer_type = get_label(questionText)\n",
    "    #print predict_answer_type\n",
    "    #result = get_ngram_nn(3,questionText,sent2)\n",
    "    return sent2\n",
    "\n",
    "\n",
    "def get_answer3(question):\n",
    "    questionText = ''.join(w for w in question if w not in PunctuationExclude) ######\n",
    "    questionText = questionText.replace(\",\", \" ,\")\n",
    "    questionText = questionText.replace(\".\", \" .\")\n",
    "    questionText = questionText.replace(\"?\", \" ?\")\n",
    "    sent_candidate = get_relevant_sentences(questionText,query_vsm)\n",
    "    sent = dev_sentences[sent_candidate[0][0]]\n",
    "    sent3 = ''.join(w for w in sent if w not in PunctuationExclude)\n",
    "    sent3 = sent3.replace(\",\", \" ,\")\n",
    "    sent3 = sent3.replace(\".\", \" .\")\n",
    "    sent3 = sent3.split()\n",
    "    answer_pos_list = stanford_pos_tagger.tag_sents([sent3])\n",
    "    i_list = find_match(questionText.split(),sent3)\n",
    "    result = find_nn(sent3,i_list)\n",
    "    if result is None:\n",
    "        print '--------'\n",
    "        result = get_first_nn(questionText,answer_pos_list)\n",
    "        print result\n",
    "    #answer = ''.join(w for w in result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# get answer for dev_dataset\n",
    "\n",
    "print len(dev_q_a) \n",
    "\n",
    "    \n",
    "def check_accuracy(guess,answer):\n",
    "    if guess == answer:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_evaluation_result(q_a_file):\n",
    "    counter = 0\n",
    "    ground_truth = q_a_file\n",
    "    for each in ground_truth:\n",
    "        guess = get_answer3(each[0])\n",
    "        #guess = get_ngram_nn(3,each[0],guess_sent)\n",
    "        print each[0], each[1]\n",
    "        print guess\n",
    "        #if check_accuracy(guess,each[1]):\n",
    "            #counter += 1\n",
    "    #return counter/len(q_a_file)\n",
    "\n",
    "print get_evaluation_result(dev_q_a)\n",
    "#sent3 = 'Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected .'.split()\n",
    "#q3 = 'What technology is used by night-vision devices ?'.split()\n",
    "#i_list = find_match(q3,sent3)\n",
    "#result = find_nn(sent3,i_list)\n",
    "#print result\n",
    "#answer_pos_list2 = stanford_pos_tagger.tag_sents([sent3.split()])  \n",
    "#print answer_pos_list2 \n",
    "#for each in sent3:\n",
    "    #print sent3.index(each)\n",
    "    #print stemmer.stem(each.lower())\n",
    "    #print answer_pos_list2.index(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
